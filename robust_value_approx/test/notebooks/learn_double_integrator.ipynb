{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import double_integrator_utils\n",
    "import robust_value_approx.value_to_optimization as value_to_optimization\n",
    "import robust_value_approx.hybrid_linear_system as hybrid_linear_system\n",
    "import robust_value_approx.model_bounds as model_bounds\n",
    "import robust_value_approx.gurobi_torch_mip as gurobi_torch_mip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "import gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data\n",
    "vf = double_integrator_utils.get_value_function()\n",
    "x0_lo = -1. * torch.ones(vf.sys.x_dim, dtype=vf.dtype)\n",
    "x0_up = 1. * torch.ones(vf.sys.x_dim, dtype=vf.dtype)\n",
    "num_breaks = [100] * vf.sys.x_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples_train, v_samples_train = vf.get_value_sample_grid(x0_lo, x0_up, num_breaks, update_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples_file = '../data/robust_value_demo_x'\n",
    "v_samples_file = '../data/robust_value_demo_v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_samples_train, x_samples_file + '_train.pt')\n",
    "torch.save(v_samples_train, v_samples_file + '_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples_train = torch.load(x_samples_file + '_train.pt')\n",
    "v_samples_train = torch.load(v_samples_file + '_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nn_width = 16\n",
    "nn_depth = 1\n",
    "learning_rate = 1e-3\n",
    "momentum = .25\n",
    "train_data_set = torch.utils.data.TensorDataset(x_samples_train,\n",
    "                                                v_samples_train)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data_set,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "nn_layers = [nn.Linear(vf.sys.x_dim, nn_width), nn.ReLU()]\n",
    "for i in range(nn_depth):\n",
    "    nn_layers += [nn.Linear(nn_width, nn_width), nn.ReLU()]\n",
    "nn_layers += [nn.Linear(nn_width, 1)]\n",
    "model = nn.Sequential(*nn_layers).double()\n",
    "l2_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=learning_rate, momentum=momentum)\n",
    "robust_model = copy.deepcopy(model)\n",
    "robust_l2_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "robust_optimizer = torch.optim.SGD(robust_model.parameters(),\n",
    "                                   lr=learning_rate, momentum=momentum)\n",
    "mb = model_bounds.ModelBounds(robust_model, vf)\n",
    "writer = SummaryWriter()\n",
    "num_total_iter_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_l2_iter_desired = 100\n",
    "num_robust_iter_desired = 100\n",
    "num_l2_iter_done = 0\n",
    "num_robust_iter_done = 0\n",
    "robust_weight = .5\n",
    "while True:\n",
    "    if num_l2_iter_done >= num_l2_iter_desired and num_robust_iter_done >= num_robust_iter_desired:\n",
    "        break\n",
    "    for batch_data, batch_label in train_data_loader:\n",
    "        # nonrobust (pure l2 regression)\n",
    "        y_pred = model(batch_data)\n",
    "        l2 = l2_fn(y_pred, batch_label)\n",
    "        if num_l2_iter_done < num_l2_iter_desired:\n",
    "            epsilon = torch.Tensor([0.]).type(vf.dtype)\n",
    "        else:\n",
    "            (Q1, Q2, q1, q2, k,\n",
    "             G1, G2, h,\n",
    "             A1, A2, b) = mb.upper_bound_opt(model, x0_lo, x0_up)\n",
    "            mb_prob = gurobi_torch_mip.GurobiTorchMIQP(vf.dtype)\n",
    "            mb_prob.gurobi_model.setParam(gurobipy.GRB.Param.OutputFlag, False)\n",
    "            # the continuous variables: y = [x, s, z]\n",
    "            y = mb_prob.addVars(Q1.shape[0], lb=-gurobipy.GRB.INFINITY,\n",
    "                                vtype=gurobipy.GRB.CONTINUOUS, name=\"y\")\n",
    "            # the binary variable: γ = [α, β]\n",
    "            gamma = mb_prob.addVars(Q2.shape[0], vtype=gurobipy.GRB.BINARY, name=\"gamma\")\n",
    "            mb_prob.setObjective([.5 * Q1, .5 * Q2],\n",
    "                                 [(y, y), (gamma, gamma)],\n",
    "                                 [q1, q2], [y, gamma], k,\n",
    "                                 gurobipy.GRB.MINIMIZE)\n",
    "            for i in range(G1.shape[0]):\n",
    "                mb_prob.addLConstr([G1[i, :], G2[i, :]], [y, gamma],\n",
    "                                   gurobipy.GRB.LESS_EQUAL, h[i])\n",
    "            for i in range(A1.shape[0]):\n",
    "                mb_prob.addLConstr([A1[i, :], A2[i, :]], [y, gamma],\n",
    "                                   gurobipy.GRB.EQUAL, b[i])\n",
    "            mb_prob.gurobi_model.update()\n",
    "            mb_prob.gurobi_model.optimize()\n",
    "            epsilon = mb_prob.compute_objective_from_mip_data_and_solution(penalty=1e-8)\n",
    "        loss = l2 + 0.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # robust regression\n",
    "        y_pred_robust = robust_model(batch_data)\n",
    "        robust_l2 = robust_l2_fn(y_pred_robust, batch_label)\n",
    "        if num_l2_iter_done < num_l2_iter_desired:\n",
    "            robust_epsilon = torch.Tensor([0.]).type(vf.dtype)\n",
    "            num_l2_iter_done += 1\n",
    "        else:\n",
    "            (Q1, Q2, q1, q2, k,\n",
    "             G1, G2, h,\n",
    "             A1, A2, b) = mb.upper_bound_opt(robust_model, x0_lo, x0_up)\n",
    "            mb_prob = gurobi_torch_mip.GurobiTorchMIQP(vf.dtype)\n",
    "            mb_prob.gurobi_model.setParam(gurobipy.GRB.Param.OutputFlag, False)\n",
    "            y = mb_prob.addVars(Q1.shape[0], lb=-gurobipy.GRB.INFINITY,\n",
    "                                vtype=gurobipy.GRB.CONTINUOUS, name=\"y\")\n",
    "            gamma = mb_prob.addVars(Q2.shape[0], vtype=gurobipy.GRB.BINARY, name=\"gamma\")\n",
    "            mb_prob.setObjective([.5 * Q1, .5 * Q2],\n",
    "                                 [(y, y), (gamma, gamma)],\n",
    "                                 [q1, q2], [y, gamma], k,\n",
    "                                 gurobipy.GRB.MINIMIZE)\n",
    "            for i in range(G1.shape[0]):\n",
    "                mb_prob.addLConstr([G1[i, :], G2[i, :]], [y, gamma],\n",
    "                                   gurobipy.GRB.LESS_EQUAL, h[i])\n",
    "            for i in range(A1.shape[0]):\n",
    "                mb_prob.addLConstr([A1[i, :], A2[i, :]], [y, gamma],\n",
    "                                   gurobipy.GRB.EQUAL, b[i])\n",
    "            mb_prob.gurobi_model.update()\n",
    "            mb_prob.gurobi_model.optimize()\n",
    "            robust_epsilon = mb_prob.compute_objective_from_mip_data_and_solution(penalty=1e-8)\n",
    "            num_robust_iter_done += 1\n",
    "        robust_loss = robust_l2 + robust_weight*torch.exp(-robust_epsilon)\n",
    "        robust_optimizer.zero_grad()\n",
    "        robust_loss.backward()\n",
    "        robust_optimizer.step()\n",
    "        # logging\n",
    "        writer.add_scalars('L2',\n",
    "                           {'l2 training': l2.item(),\n",
    "                            'robust training': robust_l2.item()},\n",
    "                           num_total_iter_done)\n",
    "        if num_l2_iter_done >= num_l2_iter_desired:\n",
    "            writer.add_scalars('Epsilon',\n",
    "                               {'l2 training': torch.clamp(-epsilon, 0.).item(),\n",
    "                                'robust training': torch.clamp(-robust_epsilon, 0.).item()},\n",
    "                               num_total_iter_done)\n",
    "        num_total_iter_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../data/robust_value_demo_model.pt')\n",
    "torch.save(robust_model, '../data/robust_value_demo_robust_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
