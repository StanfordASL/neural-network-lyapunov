\documentclass{article}
\usepackage{amsmath, amsfonts, graphicx}
\title{Synthesizing neural-network stabilizing controller for learned dynamics model}
\begin{document}
\section{Problem statement}
Assume that we are given dynamics model represented as neural networks
\begin{align}
	\text{discrete time } x[n+1] = f(x[n], u[n]) , u[n]\in\mathcal{U}\\
	\text{continuous time } \dot{x} = f(x) + G(x)u, u\in\mathcal{U}
\end{align}
where $f, G$ contain neural networks which we will describe later, our goal is to synthesize a controller $u=\pi(x)$, also represented by a neural network, and a Lyapunov function $V(x)$, represented by a third neural network, such that we can prove that the closed-loop system is Lyapunov stable. Namely we want to find a controller, such that the closed loop system is Lyapunov exponentially (or asymptotically) stable within a set $\mathcal{S}$, i.e., for the equilibrium state/control $x^*, u^*$, all the states start within $\mathcal{S}$ would eventually converge to equilibrium state.

We assume the set of admissible inputs $\mathcal{U}$ is a box in the input space, namely we have input bounds $u_{min}(i)\leq u(i)\leq u_{max}(i)$ for each dimension of the input.

Notice that we assume the continuous-time system is control affine. We will exploit this property later.

\section{Approach}
We consider the discrete-time and continuous-time system separately. Let's first consider the discrete-time case. In this project, all the neural networks are feed-forward neural network with leaky Relu units, hence the output of the network is a piecewise affine function of the input.
\subsection{Discrete time system}
First we assume that our forward dynamical system is represented by a neural network
\begin{align}
	x[n+1] = \phi_{dyn}(x[n], u[n]) - \phi_{dyn}(x^*, u^*) + x^* \label{eq:discrete_forward_dyn}
\end{align}
where $\phi_{dyn}$ is a feed-forward neural network with (leaky) ReLU activation units. Notice that by construction \eqref{eq:discrete_forward_dyn} guarantees that with $x[n]=x^*, u[n]=u^*$ the next state is still the equilibrium state $x^*$. This neural network $\phi_{dyn}$ is given and fixed.

The Lyapunov function for exponetial stability is
\begin{subequations}
\begin{align}
	V(x) > 0 \;\forall x\neq x^*, V(x^*) = 0\\
	V(x[n+1]) - V(x[n]) \le -\epsilon_2 V(x[n])\\
	x \rightarrow \infty \Rightarrow V(x)\rightarrow \infty
\end{align}
\end{subequations}
 
Since we will certify the Lyapunov condition through MILP, which cannot handle strict inequality constraint $V(x) > 0$, we consider the following necessary and sufficient condition
\begin{subequations}
\begin{align}
	V(x) \ge \epsilon_1 |R(x-x^*)|_1\\
	V(x[n+1]) - V(x[n]) \le -\epsilon_2V(x[n])
\end{align}
\label{eq:lyapunov_discrete}
\end{subequations}
where $R$ is a matrix with full column rank, $|R(x-x^*)|_1$ is the 1-norm of the vector $R(x-x^*)$.

We design our Lyapunov function as
\begin{align}
	V(x) = \phi_{V, \theta}(x) -\phi_{V, \theta}(x^*) + \lambda|R(x-x^*)|_1 \label{eq:lyapunov}
\end{align}
where $\phi_{V, \theta}$ is a feedforward neural network with (leaky) ReLU activation functions. $\lambda$ is a given positive constant (with $\lambda > \epsilon_1$). The reason to add the term  $\lambda|R(x-x^*)|_1$ to the Lyapunov function \eqref{eq:lyapunov}, is that it is very hard for the neural network $\phi_{V, \theta}$ to attain its minimum at $x^*$ (Since the neural network $\phi_{V, \theta}$ is a piecewise affine function of $x$. If it were to attain minimal at $x^*$, it implies that $x^*$ is the common vertex of all the neighbouring linear pieces, which is almost impossible to satisfy by gradient descent approach. See fig. \ref{fig:lyapunov_add_l1_3} as a visual explanation.) By adding the function $\lambda|R(x-x^*)|_1$ which has its global minimal at $x^*$, it is much easier to make the Lyapunov function to attain global minimal at $x^*$. Also by construction of \eqref{eq:lyapunov} we have $V(x^*) = 0$.
\begin{figure}
	\includegraphics[width=0.8\textwidth]{/home/hongkaidai/Dropbox/talks/pictures/neural_network_controller/lyapunov_add_l1_3.pdf}
	\caption{Adding the term $\lambda |x-x^*|_1$ helps the neural network to attain (local) minimal at $x^*$.}
	\label{fig:lyapunov_add_l1_3}
\end{figure}

Our controller is also represented by a neural network as
\begin{align}
	u[n] = saturate(\phi_{u, \eta}(x[n]) - \phi_{u, \eta}(x^*) + u^*)\label{eq:controller}
\end{align}
where $\phi_{u, \eta}$ is a feedforward neural network with (leaky) ReLU activation functions. The weights/biases of this network is denoted by $\eta$. $saturate$ is the saturation function that clamp the control within the input limits. Again by construction of \eqref{eq:controller}, the control action at the equilibrium state $x^*$ is $u^*$.

We could solve the following two optimization problem as MILP
\begin{align}
	\max_{x} \epsilon_1|R(x-x^*)|_1 - V(x)\\
	\max_{x[n]} V(x[n+1]) - V(x[n]) + \epsilon_2V(x[n])
\end{align}
When the maximal cost of either the function is larger than 0, we find a counter example that violates the Lyapunov condition \eqref{eq:lyapunov_discrete}. Our goal is to find the neural network for controller and Lyapunov function, such that the violation is 0. Namely we solve the following min-max problem.
\begin{align}
	\min_{\theta, \eta, R} \left(\max_{x}\epsilon_1|R(x-x^*)|_1 - V(x) + \max_{x[n]} V(x[n+1]) - V(x[n]) + \epsilon_2V(x[n])\right)
\end{align}
After solving the inner maximization problem, we then compute the gradient of the maximal cost w.r.t $\theta, \eta, R$, and then use gradient descent to minimize the loss.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Continuous-time system}
We assume that the continuous time system forward dynamics is
\begin{align}
	\dot{x} = f(x) + G(x)u
\end{align}
Without loss of generality we can assume that the bounds on the control input $u$ is 
\begin{align}
	-\mathbf{1}\le u \le \mathbf{1}
\end{align}

Due to the control-affine property of the continuous-time system, we will show that we can search for a control Lyapunov function, instead of a controller and a Lyapunov function as in the discrete-time case. The control Lyapunov function satisfies
\begin{subequations}
\begin{align}
	V(x) \ge \epsilon_1|R(x-x^*)|_1\\
	\min_{-\mathbf{1}\le u\le \mathbf{1}} \dot{V}\le -\epsilon_2V(x)\label{eq:lyapunov_condition_Vdot_continuous}
\end{align}
\label{eq:lyapunov_condition_continuous}
\end{subequations}

Note that the left-hand side of the condition \eqref{eq:lyapunov_condition_Vdot_continuous} can be rewritten as
\begin{subequations}
\begin{align}
	&\min_{-\mathbf{1}\le u \le \mathbf{1}} \dot{V}\\
	=&\min_{-\mathbf{1}\le u \le\mathbf{1}}\frac{\partial V}{\partial x}(f(x) + G(x)u)\\
	=&\frac{\partial V}{\partial x}f(x) + \min_{-\mathbf{1}\le u \le \mathbf{1}} \frac{\partial V}{\partial x}G(x)u\label{eq:control_lyapunov3}\\
	=&\frac{\partial V}{\partial x}f(x) - \left|\frac{\partial V}{\partial x}G(x)\right|_1\label{eq:control_lyapunov4}
\end{align}
\end{subequations}
From \eqref{eq:control_lyapunov3} to \eqref{eq:control_lyapunov4} we use the fact that $\min_{|x|_{\infty}\le 1} a^Tx = -|a|_1$, namely 1-norm is the \textit{dual norm} of $\infty$-norm.

We represent the control-Lyapunov function through a neural network as
\begin{align}
	V(x) = \phi_{V, \theta}(x) - \phi_{V, \theta}(x^*) + \lambda|R(x-x^*)|_1
\end{align}

And we solve the following two MILPs to either certify the control-Lyapunov condition, or find the counter-examples.
\begin{align}
	\max_{x} \epsilon_1|R(x-x^*)|_1 - V(x)\\
	\max_{x} \underbrace{\frac{\partial V}{\partial x}f(x) - \left|\frac{\partial V}{\partial x}G(x)\right|_1}_{\min_{-\mathbf{1}\le u \le \mathbf{1}} \dot{V}} + \epsilon_2V(x)
\end{align}

We then compute the gradient of each MILP cost w.r.t $\theta, R$, and then use gradient descent on $\theta, R$ to minimize the loss.

\subsubsection{subgradient in control Lyapunov function}
One tricky thing is that when we compute $\dot{V}$ in the control Lyapunov function, it requires the gradient $\frac{\partial V}{\partial x}$. But as we use the (leaky) ReLU unit (and $l_1$ norm), the Lyapunov function $V$ is not differentiable everywhere. Specifically both the leaky ReLU unit and the $l_1$ norm function have kinks at input equal to 0. Hence we will need to think about the subgradient of the Lyapunov function. We denote the set of subgradient at $x$ as $\mathcal{D}_V(x)$.

Our goal is that for each state, there exists a control action $u$, such that $\dot{V} < 0$ for all subgradient. Namely
\begin{align}
	\max_x \min_{-\mathbf{1}\le u \le \mathbf{1}} \max_{d\in\mathcal{D}_V(x)} d^T(f(x) + G(x)u) < 0\label{eq:max_min_max_subgradient}
\end{align}
This max-min-max problem is really hard. The problem would be a lot easier if we could switch the inner minimization and maximization, such that we end up with the following max-min problem
\begin{align}
	\max_{x, d\in\mathcal{D}_V(x)} \min_{-\mathbf{1}\le u \le \mathbf{1}} d^T(f(x)+G(x)u) < 0 \label{eq:max_min_subgradient}
\end{align}
which we know is equivalent to the following condition with only maximization.
\begin{align}
	\max_{x, d\in\mathcal{D}_V(x)}d^Tf(x) - |d^TG(x)|_1 < 0 \label{eq:max_subgradient}
\end{align}
So the question is, what is the condition such that we can switch the inner minimization and maximization from \eqref{eq:max_min_max_subgradient} to \eqref{eq:max_min_subgradient}?

From \textit{mini-max} theorem we know that if the set of subgradient $\mathcal{D}_V(x)$ is a convex compact set, then we can switch the order of the min-max. We will next describe some conditions such that the subgradient set $\mathcal{D}_V(x)$ is a convex compact set.

Note that
\begin{align}
	V(x) = \phi_V(x) - \phi_V(x^*) + \lambda |R(x-x^*)|_1
\end{align}
Hence the subgradient set $\mathcal{D}_V(x)$ can be written as the Minkowski sum $\mathcal{D}_\phi(x) \oplus \mathcal{D}_l(x)$, where $\mathcal{D}_\phi(x)$ is the set of subgradient for the neural-network $\phi_V(x)$, and $\mathcal{D}_l(x)$ is the set of subgradient for the $l_1$ norm function $l(x) = \lambda |R(x-x^*)|_1$. Let's first analyze the subgradient for the $l_1$ norm function. The set $\mathcal{D}_l(x)$ can be described as
\begin{align}
	\mathcal{D}_l(x) = \{\lambda y^TR\}
\end{align}
where the i'th entry of $y$ has the following form
\begin{align}
	y(i)\begin{cases}
		= 1 & \text{ if } e_i^TR(x - x^*) > 0\\
		= -1 & \text{ if }e_i^TR(x - x^*) < 0\\
		\in [-1, 1] &\text { if } e_i^TR(x - x^*) = 0
	\end{cases}
\end{align}
Hence for a fixed $x$, the set $\mathcal{Y} = \{y\}$ is a convex compact set. As a result, $\mathcal{D}_l(x)$, which is a linear transformation of $\mathcal{Y}$, also becomes a convex compact set.

Similarly if we consider the ReLU network. The set of subgradient for each individual ReLU unit is also a convex set. Hence if only one layer of neurons has non-unique subgradient, while all other layers have unique gradient, then the set of subgradient $\mathcal{D}_V(x)$ for the entire neural network is also a convex compact set.

To summarize, a sufficient condition for the subgradient set $\mathcal{D}_V(x)$ to be convex and compact, is that for the given state $x$, the (leaky) ReLU units with input being 0 are all in the same layer of the neural network.

\subsubsection{Computing subgradient}
We use the $l_1$ norm function $|R(x-x^*)|_1$ to denote how to use the subgradient. We will have the terms $\frac{\partial |R(x-x^*)|_1}{\partial x}f(x)$ and $\frac{\partial |R(x-x^*)|_1}{\partial x}G(x)$ when computing $\dot{V}$. If we denote a valid subgradient as $g$, then these two terms are written as $g^TRf, g^TRG$, where the value of $g$ is
\begin{align}
	g_i  \begin{cases}
		=1 \text{ if } R[i, :](x-x^*) > 0\\
		=-1 \text{ if } R[i, :](x-x^*) < 0\\
		\in[-1, 1] \text{ if } R[i, :](x-x^*)=0
	\end{cases}
\end{align}
Notice that when the gradient is unique, then we only need to consider the discrete value $\pm1$ multiplying the continuous value $Rf, RG$, and this product can be captured by mixed-integer linear constraints. But when we have subgradient, then we need to consider the continuous variable in the range $[-1, 1]$ multiplying continuous value $Rf, RG$. This product between continuous variables can't be captured by mixed-integer linear constraints.

To remedy this, we only consider a finite number of sampled subgradients. For example if we only consider the sub-gradient $-1, 0, 1$, then we can write the subgradient as
\begin{align}
	g_i = \beta_0*-1+\beta_1 * 0 + \beta_2 * 1, \beta_i \text{ is binary}, \beta_0 + \beta_1 + \beta_2 = 1 \text{ if } R[i, :] (x-x^*)=0
\end{align}

\end{document}
